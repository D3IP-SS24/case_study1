{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZzQrlYHFI-c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Data Driven Decisions in Practice (D3IP): Urban Analytics*\n",
    "\n",
    "# Case Study: Predicting AirBnB Accomodation Prices\n",
    "\n",
    "Gunther Gust & Nikolai Stein\n",
    "\n",
    "Data Driven Decisions (D3) Group <br>\n",
    "Center for Artificial Intelligence & Data Science <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/d3.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/CAIDASlogo.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKaui31bFI-d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Complete the case study outlined by the steps below. Remember to always comment your code and document your findings so that your notebook is easy to read and follow! (Apart from correctness, the style of the notebook will also affect your grade!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Data Loading and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insructions:\n",
    "\n",
    "* Load the `airbnb.geojson` file into this notebook as a geodataframe\n",
    "* Explore the content of each column of the geodataframe using methods of your choice (descriptive statistics, plots etc.). Describe your findings in the markdown cells.\n",
    "* Create a plot that displays the location of the airbnb listings and the price. Add a basemap of San Diego to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geodataframe\n",
    "db = gpd.read_file(\"airbnb_listings.geojson\")\n",
    "\n",
    "#exploring the dataset...  \n",
    "f, ax = plt.subplots(1, figsize=(16, 16))\n",
    "db.plot(ax = ax, column = 'price', legend = True, alpha = 0.5, cmap = 'inferno', markersize = 10)\n",
    "cx.add_basemap(ax, crs = db.crs, source=cx.providers.Esri.WorldImagery)\n",
    "plt.show()                                                                                          #...by plotting the accommodations with their respective price\n",
    "\n",
    "sns.histplot(db['price'])                                                                           #...alternatively as a histogram to visualize the distribution of the prices\n",
    "\n",
    "db.describe()                                                                                       #...and using the describe function to look at the statistics of the geodataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations deducted from the shown plot, table and histogram:\n",
    "- most of the AirBnB accomodations are relatively cheap (75% of the listings cost up to 250$) and are clustered on the lower end of the entire scale of given prices\n",
    "- only a few outliers exist (the highest price of an accommodation is 4900$)\n",
    "- the accommodations in the upper 25% of the price range are usually located near the coast or beach area\n",
    "- there is a circular shaped cluster on the right that is hollowed out (Balboa Park is located their and the accommodations are clustered around it, hence this hollowed out ring like shape)\n",
    "- the area called \"La Jolla\" contains most of the expensive accommodations of the upper 25% of the given price range "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the variables you should analyze and later use to predict the `price`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variable_names = [\n",
    "    \"accommodates\",  # Number of people it accommodates\n",
    "    \"bathrooms\",  # Number of bathrooms\n",
    "    \"bedrooms\",  # Number of bedrooms\n",
    "    \"beds\",  # Number of beds\n",
    "    # Below are binary variables, 1 True, 0 False\n",
    "    \"rt_Private_room\",  # Room type: private room\n",
    "    \"rt_Shared_room\",  # Room type: shared room\n",
    "    \"pg_Condominium\",  # Property group: condo\n",
    "    \"pg_House\",  # Property group: house\n",
    "    \"pg_Other\",  # Property group: other\n",
    "    \"pg_Townhouse\",  # Property group: townhouse\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 Feature Engineering: Get points-of-interest (POIs) and prepare them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "* Use the method `features_from_address()` of osmnx to download POIs of the type `amenity` (select the types of amenities to include from this list here: https://wiki.openstreetmap.org/wiki/Key:amenity)\n",
    "* Pay attention to set the `dist` parameter to an appropiate value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the POI data\n",
    "#for plotting the POIs with all chosen amenity types included\n",
    "pois = ox.features_from_address(\"San Diego, USA\", dist = 5000, tags = {\"amenity\" : [\"bar\", \"atm\", \"cafe\", \"restaurant\"]})\n",
    "\n",
    "#to calculate the KDE estimates for each POI amenity type separately\n",
    "poiBars = ox.features_from_address(\"San Diego, USA\", dist = 5000, tags = {\"amenity\" : \"bar\"})\n",
    "poiATMs = ox.features_from_address(\"San Diego, USA\", dist = 5000, tags = {\"amenity\" : \"atm\"})\n",
    "poiCafes = ox.features_from_address(\"San Diego, USA\", dist = 5000, tags = {\"amenity\" : \"cafe\"})\n",
    "poiRestaurants = ox.features_from_address(\"San Diego, USA\", dist = 5000, tags = {\"amenity\" : \"restaurant\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning: The resulting `pois` geodataframe may have a composite row index. In addition, some POIs may be of a strange element_type. You can use the following code to eliminate these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the POI data\n",
    "pois.reset_index(inplace=True) # reset the index of the data frame\n",
    "pois = pois[pois.element_type==\"node\"] # eliminate all POIs that are not of the type \"node\"\n",
    "\n",
    "\n",
    "#Clean POI bei amenity\n",
    "#Bar POI amenity type\n",
    "poiBars.reset_index(inplace=True)\n",
    "poiBars = poiBars[poiBars.element_type == \"node\"]\n",
    "\n",
    "\n",
    "#ATM POI amenity type\n",
    "poiATMs.reset_index(inplace=True)\n",
    "poiATMs = poiATMs[poiATMs.element_type == \"node\"]\n",
    "\n",
    "\n",
    "#Cafe POI amenity type\n",
    "poiCafes.reset_index(inplace=True)\n",
    "poiCafes = poiCafes[poiCafes.element_type == \"node\"]\n",
    "\n",
    "\n",
    "#Restaurant POI amenity type\n",
    "poiRestaurants.reset_index(inplace=True)\n",
    "poiRestaurants = poiRestaurants[poiRestaurants.element_type == \"node\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "* Plot the POIs spatially (use again a background map of San Diego)\n",
    "* When you use POIs of different amenity types, color the POIs differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fAllPOIs, ax = plt.subplots(1)\n",
    "pois.plot(alpha = 1, ax = ax, edgecolor = 'black', column = 'amenity', legend = True, cmap = 'plasma')\n",
    "cx.add_basemap(ax, crs = db.crs)\n",
    "ax.set_title(\"Different amenity types in San Diego\")\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each POI amenity type, create a kernel density estimation (KDE):\n",
    "* Convert the `geometry` of the POI into a suitable coordinate data format (you may use the provided function `create_coordinate_array` for this)\n",
    "* Feed the resulting coordinates into the `gaussian_kde` function and estimate the function\n",
    "* Also convert the `geometry` of the Airbnb listings into the coordinate data format (you may use the provided function `create_coordinate_array` for this)\n",
    "* Using the converted Airbnb geometries, compute the KDE for the locations of the Airbnb listings\n",
    "* Add the KDE estimate as additional columns to your original airbnb geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert the geometries into a suitable coordinate format for the KDE\n",
    "def create_coordinate_array(geometries): \n",
    "    x_values = []\n",
    "    y_values = []\n",
    "\n",
    "# Iterate through each row in the GeoDataFrame\n",
    "    for multipoint in geometries:\n",
    "        # Ensure the geometry is indeed MultiPoint; if it's just a single Point, wrap it in a list\n",
    "        points = list(multipoint.geoms) if hasattr(multipoint, \"geoms\") else [multipoint]\n",
    "        \n",
    "        # For each Point in the MultiPoint, extract x and y values\n",
    "        for point in points:\n",
    "            x_values.append(point.x)\n",
    "            y_values.append(point.y)\n",
    "\n",
    "    # Optionally, convert the lists to numpy arrays for further processing\n",
    "    x_values = np.array(x_values)\n",
    "    y_values = np.array(y_values)\n",
    "\n",
    "    # Rearrange data to create a 2D array of x and y coordinates\n",
    "    xy = np.vstack([x_values,y_values])\n",
    "\n",
    "    return xy\n",
    "\n",
    "\n",
    "# Example Usage for the Airbnb geodataframe\n",
    "#airbnb_array = create_coordinate_array(airbnb.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating KDE estimates with all POI amenity types included and for each amenity type individually for plotting\n",
    "\n",
    "#WARNING#\n",
    "#we ran into a ValueError when trying to add the calculated densities to the dataframe\n",
    "#to not crash the notebook when executing, the problematic lines have been commented-out because we were unable to find the source of this error \n",
    "\n",
    "# Create coordinate array\n",
    "xyDB = create_coordinate_array(db.geometry)\n",
    "xyBars = create_coordinate_array(poiBars.geometry)\n",
    "xyATMs = create_coordinate_array(poiBars.geometry)\n",
    "xyCafes = create_coordinate_array(poiCafes.geometry)\n",
    "xyRestaurants = create_coordinate_array(poiRestaurants.geometry)\n",
    "\n",
    "\n",
    "# Compute the density estimation\n",
    "kdeDB = gaussian_kde(xyDB)\n",
    "kdeBars = gaussian_kde(xyBars)\n",
    "kdeATMs = gaussian_kde(xyATMs)\n",
    "kdeCafes = gaussian_kde(xyCafes)\n",
    "kdeRestaurants = gaussian_kde(xyRestaurants)\n",
    "\n",
    "\n",
    "# Defining the grid points \n",
    "xminDB, xmaxDB = xyDB[0].min(), xyDB[0].max()\n",
    "yminDB, ymaxDB = xyDB[1].min(), xyDB[1].max()\n",
    "\n",
    "xminBars, xmaxBars = xyBars[0].min(), xyBars[0].max()\n",
    "yminBars, ymaxBars = xyBars[1].min(), xyBars[1].max()\n",
    "\n",
    "xminATMs, xmaxATMs = xyATMs[0].min(), xyATMs[0].max()\n",
    "yminATMs, ymaxATMs = xyATMs[1].min(), xyATMs[1].max()\n",
    "\n",
    "xminCafes, xmaxCafes = xyCafes[0].min(), xyCafes[0].max()\n",
    "yminCafes, ymaxCafes = xyCafes[1].min(), xyCafes[1].max()\n",
    "\n",
    "xminRestaurants, xmaxRestaurants = xyRestaurants[0].min(), xyRestaurants[0].max()\n",
    "yminRestaurants, ymaxRestaurants = xyRestaurants[1].min(), xyRestaurants[1].max()\n",
    "\n",
    "#create 100 evenly spaced points between min and max\n",
    "xxDB, yyDB = np.mgrid[xminDB:xmaxDB:100j, yminDB:ymaxDB:100j]\n",
    "xxBars, yyBars = np.mgrid[xminBars:xmaxBars:100j, yminBars:ymaxBars:100j]\n",
    "xxATMs, yyATMs = np.mgrid[xminATMs:xmaxATMs:100j, yminATMs:ymaxATMs:100j]\n",
    "xxCafes, yyCafes = np.mgrid[xminCafes:xmaxCafes:100j, yminCafes:ymaxCafes:100j]\n",
    "xxRestaurants, yyRestaurants = np.mgrid[xminRestaurants:xmaxRestaurants:100j, yminRestaurants:ymaxRestaurants:100j]\n",
    "\n",
    "# Evaluate the density at grid points\n",
    "densityDB = kdeDB(np.vstack([xxDB.ravel(), yyDB.ravel()]))\n",
    "densityDB = densityDB.reshape(xxDB.shape) # reshape to the original shape of xx (for plotting)\n",
    "#db['KDE estimate of all POIs'] = densityDB\n",
    "\n",
    "densityBars = kdeBars(np.vstack([xxBars.ravel(), yyBars.ravel()]))\n",
    "densityBars = densityBars.reshape(xxBars.shape) # reshape to the original shape of xx (for plotting)\n",
    "#db['KDE estimate of POI amenity type: Bar'] = densityBars\n",
    "\n",
    "densityATMs = kdeATMs(np.vstack([xxATMs.ravel(), yyATMs.ravel()]))\n",
    "densityATMs = densityATMs.reshape(xxATMs.shape) # reshape to the original shape of xx (for plotting)\n",
    "#db['KDE estimate of POI amenity type: ATM'] = densityATMs\n",
    "\n",
    "densityCafes = kdeCafes(np.vstack([xxCafes.ravel(), yyCafes.ravel()]))\n",
    "densityCafes = densityCafes.reshape(xxCafes.shape) # reshape to the original shape of xx (for plotting)\n",
    "#db['KDE estimate of POI amenity type: Cafe'] = densityCafes\n",
    "\n",
    "densityRestaurants = kdeRestaurants(np.vstack([xxRestaurants.ravel(), yyRestaurants.ravel()]))\n",
    "densityRestaurants = densityRestaurants.reshape(xxRestaurants.shape) # reshape to the original shape of xx (for plotting)\n",
    "#db['KDE estimate of POI amenity type: Restaurant'] = densityRestaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spatial point plot(s) of the Airbnb listings and color the points according to the KDE estimates, in order to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.rot90(densityDB), cmap='hot', extent=[xminDB, xmaxDB, yminDB, ymaxDB])\n",
    "plt.colorbar(label='Density')\n",
    "plt.scatter(xyDB[0], xyDB[1], s=10, c='blue', alpha=0.5)\n",
    "plt.title('Density of the AirBnB accommodations')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.rot90(densityBars), cmap='hot', extent=[xminBars, xmaxBars, yminBars, ymaxBars])\n",
    "plt.colorbar(label='Density')\n",
    "plt.scatter(xyBars[0], xyBars[1], s=10, c='blue', alpha=0.5)\n",
    "plt.title('KDE for POI amenity type: Bar')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.rot90(densityATMs), cmap='hot', extent=[xminATMs, xmaxATMs, yminATMs, ymaxATMs])\n",
    "plt.colorbar(label='Density')\n",
    "plt.scatter(xyATMs[0], xyATMs[1], s=10, c='blue', alpha=0.5)\n",
    "plt.title('KDE for POI amenity type: ATM')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.rot90(densityCafes), cmap='hot', extent=[xminCafes, xmaxCafes, yminCafes, ymaxCafes])\n",
    "plt.colorbar(label='Density')\n",
    "plt.scatter(xyCafes[0], xyCafes[1], s=10, c='blue', alpha=0.5)\n",
    "plt.title('KDE for POI amenity type: Cafe')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.rot90(densityRestaurants), cmap='hot', extent=[xminRestaurants, xmaxRestaurants, yminRestaurants, ymaxRestaurants])\n",
    "plt.colorbar(label='Density')\n",
    "plt.scatter(xyRestaurants[0], xyRestaurants[1], s=10, c='blue', alpha=0.5)\n",
    "plt.title('KDE for POI amenity type: Restaurant')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: Modeling and Analysis (aka Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your dataset generated in the previous steps to predict AirBnb prices\n",
    "\n",
    "Instructions:\n",
    "\n",
    "* Split your data set into training and validation data sets\n",
    "* Define an error metric (or several)\n",
    "* Train at least one machine learning model (e.g. random forest) \n",
    "* Tune the hyperparameters (if applicable for the model)\n",
    "* Evaluate the accuracy of the predicted prices against actual prices\n",
    "* Compare the performance of the previous models when using different input data sets (benchmarks). Make sure to include the naive benchmark of predicting always the mean price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "#removing missing values\n",
    "db.dropna(axis=0, inplace=True)\n",
    "\n",
    "#prediction target\n",
    "y = db['price']\n",
    "\n",
    "#features\n",
    "#in the following lines we tested different feature inputs and there effects on the mae (e.g. with or without property group)\n",
    "db_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'rt_Private_room', 'rt_Shared_room', 'pg_Condominium', 'pg_House', 'pg_Other', 'pg_Townhouse']\n",
    "#db_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'pg_Condominium', 'pg_House', 'pg_Other', 'pg_Townhouse']\n",
    "#db_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'rt_Private_room', 'rt_Shared_room']\n",
    "#db_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds']\n",
    "#db_features = ['accommodates', 'bathrooms', 'beds']\n",
    "\n",
    "X = db[db_features]\n",
    "\n",
    "#split data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "#Funktion für mae vergleich\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all property group variables had no impact on the results.\n",
    "\n",
    "Removing all room type variables improved the mae score. \n",
    "\n",
    "Removing all property group variables and all room type variables achieved a worse mae score.\n",
    "\n",
    "Removing numeric variables has a much higher negative impact on the mae score than removing binary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and fit model\n",
    "model_one = DecisionTreeRegressor(random_state=1) #durch die 1 immer gleiche Ergebnisse\n",
    "model_one.fit(train_X, train_y)\n",
    "\n",
    "#predict and mae \n",
    "val_predictions = model_one.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "\n",
    "#mae vergleich zwischen in-sample und out-of-sample mithilfe von Funktion\n",
    "for max_leaf_nodes in [2, 5, 50, 500, 5000, 10000]:\n",
    "    is_mae = get_mae(max_leaf_nodes, X, X, y, y)\n",
    "    oos_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t In-sample:  %d \\t Out-of-sample:  %d\" %(max_leaf_nodes, is_mae, oos_mae))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree model is just a basic model to get a view first mae score samples.\n",
    "Not very effective to achieve the best mae score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test lists\n",
    "n_list = [50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
    "m_list = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "y = 10\n",
    "print(\"\")\n",
    "print(\"varying n_estimator to find best mae score\")\n",
    "for i in n_list:\n",
    "    \n",
    "    # Define and fit\n",
    "    forest_model = RandomForestRegressor(random_state=1, n_estimators=i, max_depth=y) #n_estimators = Anzahl Bäume, max_depth = max Tiefe Bäume (Gefahr Overfitting)\n",
    "    forest_model.fit(train_X, train_y)\n",
    "    # Evaluatio\n",
    "    forest_predictions = forest_model.predict(val_X) \n",
    "    print(\"The MAE of our model with a n_estimator of {} and a max depth of {} is: {}\".format(i, y, mean_absolute_error(val_y, forest_predictions)))\n",
    "\n",
    "x = 100\n",
    "print(\"\")\n",
    "print(\"varying max_depth to find best mae score\")\n",
    "for j in m_list:        \n",
    "    \n",
    "    # Define and fit\n",
    "    forest_model = RandomForestRegressor(random_state=1, n_estimators=x, max_depth=j) #n_estimators = Anzahl Bäume, max_depth = max Tiefe Bäume (Gefahr Overfitting)\n",
    "    forest_model.fit(train_X, train_y)\n",
    "    # Evaluatio\n",
    "    forest_predictions = forest_model.predict(val_X) \n",
    "    print(\"The MAE of our model with a n_estimator of {} and a max depth of {} is: {}\".format(x, j, mean_absolute_error(val_y, forest_predictions)))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1, n_estimators=150, max_depth=8) #n_estimators = Anzahl Bäume, max_depth = max Tiefe Bäume (Gefahr Overfitting)\n",
    "forest_model.fit(train_X, train_y)\n",
    "#Evaluatio\n",
    "forest_predictions = forest_model.predict(val_X) \n",
    "print(\"The best MAE of our model with a n_estimator of 150 and a max depth of 8 is: {}\".format(mean_absolute_error(val_y, forest_predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model uses many trees and makes a prediction by the average. It has a much better accuracy than the previous model.\n",
    "\n",
    "You can adjust many different hyperparameters. For test purposes we played around with two of them.\n",
    "First the n_estimators (number of trees) and second the max_depth (max depth of the trees).\n",
    "\n",
    "Our tests revealed following results:\n",
    "\n",
    "The impact of different n_estimators is very low. \n",
    "150 achieves the best mae score\n",
    "\n",
    "A higher max_depth causes overfitting.\n",
    "a lower max_depth improved the mae. \n",
    "8 achieves the best mae score\n",
    "\n",
    "best mae: 94.9671\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit\n",
    "linear_model = LinearRegression() #keine Hyperparameter\n",
    "linear_model.fit(train_X, train_y)\n",
    "\n",
    "#Evaluate\n",
    "linear_predictions = linear_model.predict(val_X)\n",
    "print(\"The MAE of our model is: {}\".format(mean_absolute_error(val_y, linear_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the linear regression model we get only one result because we dont have any tunable hyperparameters.\n",
    "\n",
    "best mae: 104.3475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Huber Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#less sensitive to outliers\n",
    "\n",
    "#test list\n",
    "epsilon_list = [1.00, 1.05, 1.10, 1.15, 1.20, 1.25, 1.30, 1.35, 1.40, 1.45, 1.50, 2.0, 3.0, 5.0, 10.0]\n",
    "\n",
    "for i in epsilon_list:\n",
    "    # Define and fit\n",
    "    huber_model = HuberRegressor(epsilon=i) #epsilon = Steuert Robustheit\n",
    "    huber_model.fit(train_X, train_y)\n",
    "\n",
    "    #Evaluate\n",
    "    huber_predictions = huber_model.predict(val_X)\n",
    "    print(\"The MAE of our model using an epsilon of {} is: {}\".format(i, mean_absolute_error(val_y, huber_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big advantage of the huber regressor is that this model is less sensitive to outliers. In our example we have less very expensive airbnbs which are outliers. \n",
    "Because of this the huber regressor is a very good model for our example and our tests.\n",
    "\n",
    "Like the random forest model this model also has many adjustable hyperparameters.\n",
    "For the tests we chose the hyperparameter epsilon which adjusts the robustness.\n",
    "\n",
    "Our tests revealed following results:\n",
    "\n",
    "a lower epsilon value improved the mae.\n",
    "1.0 achieves the best mae score.\n",
    "\n",
    "best mae: 94.6993"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'>WS 23/24</br>Smart Cities & GSDA</div><div class='logo'><img src='images/unilogo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
